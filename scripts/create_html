#!/usr/bin/env python2
# -*- python -*-

import logging
import os, sys, glob, socket, shutil
from optparse import OptionParser
import time
import datetime
from numpy import array, median, floor

def initialize_logging(log_level, log_dir = '/globaldata/inspect/'):
    r'''
    Initialize the Python logging system. The log file will be written
    to ``/globaldata/inspect/create_html.log``.

    **Parameters**

    log_level : string
        Minimum log level of lines to write into the
        log file. Possible values are 'DEBUG', 'INFO', 'WARNING', and
        'ERROR'.

    log_dir : string
        Directory where the log file must be written.


    **Returns**

    A string containing the log file name.

    **Examples**

    >>> initialize_logging(log_level = 'INFO', log_dir='test-output')
    'test-output/create_html.log'
    '''
    log_levels   = {'DEBUG'  : logging.DEBUG,
                    'INFO'   : logging.INFO,
                    'WARNING': logging.WARNING,
                    'ERROR'  : logging.ERROR}
    level = log_levels[log_level]
    
    log_format     = ('create_html@' + socket.gethostname() +
                      ' %(asctime)s %(levelname)8s - %(message)s')
    log_file_name  = os.path.join(log_dir, 'create_html.log')

    logger       = logging.root
    for handler in logger.handlers:
        logger.removeHandler(handler)

    logger.setLevel(level)
    formatter    = logging.Formatter(log_format)

    file_handler = logging.FileHandler(log_file_name)
    logger.addHandler(file_handler)
        
    for handler in logger.handlers:
        handler.setFormatter(formatter)
        handler.setLevel(level)
    logging.debug('Initialized logging')
    return log_file_name


if __name__ == '__main__':
    initialize_logging('INFO')

try:
    import matplotlib
    matplotlib.use('agg') # Prevent initialisation of GUI system
    from pyautoplot import __version__
except ImportError:
    if __name__ == '__main__':
        logging.warning(str(sys.exc_info()[1]))
        logging.warning('Command line: %s', ' '.join(sys.argv))
    __version__ = 'unknown'

if __name__ == '__main__':
    logging.debug('Using pyautoplot version %s', __version__)
    logging.debug('Environment:\n%s',
                  '\n'.join(['%15s: %s' % 
                             (key.ljust(15), os.environ[key])
                             for key in os.environ.keys()]))




    


def create_html_main(argv):
    r'''
    The program's main function.

    **Parameters**
    
    argv : list of strings
        The contents of ``sys.argv``

    **Returns**
    
    An integer, where success is indicated by the value 0.

    **Examples**

    >> create_html_main(['create_html', '60807', '60806'])
    '''
    logging.debug('create_html_main(argv = %r)', argv)
    options, sas_ids = parse_command_line(argv)
    parset_names = [find_parset(sas_id) for sas_id in sas_ids]
    parsets = [parset_summary(sas_id, parset_name)
               for sas_id, parset_name in zip(sas_ids, parset_names)]

    for num, parset in enumerate(sorted(parsets)[::-1]):
        logging.debug('Processing SAS ID: %s', parset['sas_id'])
        try:
            obs_plot_root = os.path.join(options.plot_root, parset['sas_id'])

            if parset['file_name'] and os.path.exists(parset['file_name']):
                shutil.copy2(parset['file_name'], obs_plot_root)
                new_parset_name = os.path.join(
                    obs_plot_root,
                    os.path.basename(parset['file_name']))
                parset['file_name'] = new_parset_name
            file_size_data = None
            file_sizes_txt = os.path.join(obs_plot_root, 'file-sizes.txt')
            if os.path.exists(file_sizes_txt):
                file_sizes_dict = parse_file_sizes(file_sizes_txt)
                file_size_data  = file_size_analysis(parset, file_sizes_dict)

            observation_html(parset,
                             file_size_data,
                             obs_plot_root = obs_plot_root,
                             html_root     = options.html_root)
        except IOError:
            logging.error(str(sys.exc_info()[1]))

        if num % 10 == 0:
            main_index_html(plot_root = options.plot_root,
                            html_root = options.html_root)

            
    main_index_html(plot_root = options.plot_root,
                    html_root = options.html_root)
    return 0



def parse_command_line(argv):
    r'''
    Parse the command line arguments.

    **Parameters**
    
    argv : list of strings
        The contents of ``sys.argv``

    **Example**

    >>> parse_command_line(['create_html'])
    Traceback (most recent call last):
    ...
    ValueError: Specify at least one SAS ID

    >>> opts, sas = parse_command_line(['create_html', '1232456'])
    >>> str(opts)
    "{'plot_root': '/globaldata/inspect', 'html_root': '/globaldata/inspect/HTML'}"
    >>> sas
    [1232456]
    >>> opts, sas = parse_command_line(['create_html', '--plot-root', '/home/user/plots',
    ...                                 '--html-root', '/home/user/public_html',
    ...                                 '1232456', '6542311'])
    >>> str(opts)
    "{'plot_root': '/home/user/plots', 'html_root': '/home/user/public_html'}"
    >>> sas
    [1232456, 6542311]

    '''
    logging.debug('parse_command_line(argv = %r)', argv)
    parser = OptionParser(usage   = '%prog <sas ID>',
                          version = '%prog (pyautoplot ' + __version__ + ')')

    parser.add_option(
        '--plot-root', type = 'string',
        dest    = 'plot_root',
        help    = 'Plots are found under DIR/<sasid> (default: %default)',
        metavar = 'DIR',
        default = os.path.join('/', 'globaldata', 'inspect'))

    parser.add_option(
        '--html-root', type = 'string',
        dest    = 'html_root',
        help    = 'Use DIR for HTML output (default: %default)',
        metavar = 'DIR',
        default = os.path.join('/', 'globaldata', 'inspect', 'HTML'))

    (options, args) = parser.parse_args(argv[1:])
    
    if len(args) == 0:
        raise ValueError('Specify at least one SAS ID')
    sas_ids = [int(arg) for arg in args]
    return options, sas_ids



def subband_from_file_name(plot_file_name):
    r'''
    Extract sub band name from ``plot_file_name``.
    
    **Parameters**

    plot_file_name : string
        File name of an inspection plot.

    **Returns**
    
    A string containing the sub band identifier with format 'SBnnn',
    where ``nnn`` are three digits.

    **Examples**

    >>> subband_from_file_name('/globaldata/inspect/60873/L60873_SAP000_SB049_uv-flags.png')
    'SB049'
    '''
    return [item for item in plot_file_name.split('_')
            if item[0:2] == 'SB' and len(item) == 5][0]



def sap_from_file_name(plot_file_name):
    r'''
    Extract sub array pointing name from ``plot_file_name``.
    
    **Parameters**

    plot_file_name : string
        File name of an inspection plot.

    **Returns**
    
    A string containing the SAP identifier with format 'SAPnnn',
    where ``nnn`` are three digits.

    **Examples**

    >>> sap_from_file_name('/globaldata/inspect/60873/L60873_SAP003_SB049_uv-flags.png')
    'SAP003'
    '''
    return [item for item in plot_file_name.split('_')
            if item[0:3] == 'SAP' and len(item) == 6][0]



def force_mkdir(path_name):
    r'''
    '''
    logging.debug('force_mkdir(path_name = %r)', path_name)
    if not os.path.exists(path_name):
        os.mkdir(path_name)

 
def contains(sub_string, strings):
    r'''
    '''
    return sorted([string for string in strings if sub_string in string])




def plot_html(plot_path, image_width = 700):
    r'''
    '''
    return '''<a href="%(plot_path)s">
    <img src="%(plot_path)s" width="%(image_width)dpx"></img>
</a>''' % {'plot_path'  : plot_path,
           'image_width': image_width}


def subband_html(parset, sb_name, rel_plot_files):
    r'''
    '''
    timeseries_html    = [plot_html(image)
                          for image in contains('timeseries', rel_plot_files)]
    sensitivities_html = [plot_html(image)
                          for image in contains('station-gain', rel_plot_files)]
    
    overview_rows = ['<tr><td>'+line_graph+'</td><td>'+bar_graph+'</td></tr>'
                     for (line_graph, bar_graph)
                     in zip(timeseries_html, sensitivities_html)]
    flagged_std   = plot_html(contains('flagged-standard-deviation',
                                       rel_plot_files)[0])
    flagged_mean  = plot_html(contains('flagged-mean', rel_plot_files)[0])
    flags         = plot_html(contains('flags'       , rel_plot_files)[0])
    zeros         = plot_html(contains('zeroes'      , rel_plot_files)[0])

    html_template = '''
<html>
    <head> <title}%(title)s</title> </head>
    <body>
    <center>
        <h1>%(title)s</h1>


        <p>
          <ul>
            <li><a href="../index.html">Subbands</a> <a href="../../index.html">Projects</a></li>
            <li><a href="http://www.lofar.org/wiki/doku.php?id=operator:system_validation_observations#lofar_inspection_page">What do I see here?</a></li>
          </ul>
        </p>

        <table>
        %(overview_rows)s
        <tr>
            <td>%(flagged_std)s</td>
            <td>%(flagged_mean)s</td>
        </tr>
        <tr>
            <td>%(flags)s</td>
            <td>%(zeros)s</td>
        </tr>
        </table>
    </center>
    </body>
</html>'''

    return html_template % {
        'title'        : parset['sas_id']+' '+sb_name,
        'overview_rows': '\n'.join(overview_rows),
        'flagged_std'  : flagged_std,
        'flagged_mean' : flagged_mean,
        'flags'        : flags,
        'zeros'        : zeros}



def relative_plot_paths(obs_plot_root, current_directory):
    r'''
    '''
    patterns        = [os.path.join(obs_plot_root, extension)
                       for extension in ['*.png', '*.jpg']]
    rel_plot_files = []
    for pattern in patterns:
        rel_plot_files += [os.path.relpath(full_path, current_directory)
                           for full_path in glob.glob(pattern)]
    return rel_plot_files



def plot_paths_per_sap_subband(plot_file_names):
    r'''
    '''
    sb_plots = {}
    for plot_file in plot_file_names:
        keyword = sap_from_file_name(plot_file)+'_'+subband_from_file_name(plot_file)
        if not sb_plots.has_key(keyword):
            sb_plots[keyword] = []
        sb_plots[keyword].append(plot_file)
    return sb_plots


def observation_html(parset, file_size_data, obs_plot_root, html_root):
    r'''
    '''
    logging.debug(
        'observation_html(parset = %r, obs_plot_root = %r, html_root = %r)',
        parset, obs_plot_root, html_root)
    obs_html_root = os.path.join(html_root    , parset['sas_id'])
    sb_html_root  = os.path.join(obs_html_root, 'SBpages')

    if not os.path.exists(obs_plot_root):
        raise IOError('Directory "%s" does not exist' % obs_plot_root)
    
    force_mkdir(obs_html_root)
    force_mkdir(sb_html_root)
    
    file_size_html = '<h3>No file size information found</h3>'
    try:
        if file_size_data is None:
            raise RuntimeError('No file size data provided')
        if len(file_size_data['missing_data_sets']) == 0:
            missing_html = '<h3>All data sets are there</h3>'
        else:
            missing_html = '<h3>Missing data:</h3><ul>'+'<li>'+ '</li><li>'.join(file_size_data['missing_data_sets'])+'</li></ul>'
        
        if len(file_size_data['odd_sized_data_sets']) == 0:
            odd_sized_html = ''
        else:
            odd_sized_html = '<h3>Odd size data:</h3><table>'+'<tr>' +'</tr><tr>'.join(['<td>%s</td><td>:</td><td>%d MB</td>' % (name, size) for (name, size) in file_size_data['odd_sized_data_sets']])+'</tr></table>'
            
        file_size_html = '''
        
        <h3>Median file sizes (MB):</h3>
        <table>
        <tr><td>Correlated data</td> <td>:</td> <td>%r</td></tr>
        <tr><td>Beamformed data</td> <td>:</td> <td>%r</td></tr>
        </table>
        
        %s

        %s
        ''' % (file_size_data['median_ms_size_mb'],
               file_size_data['median_raw_size_mb'],
               missing_html,
               odd_sized_html)
    except:
        logging.error('%s: %s', str(sys.exc_info()[0].__name__), str(sys.exc_info()[1]))
    

    plot_files = relative_plot_paths(obs_plot_root, sb_html_root)
    sb_plots   = plot_paths_per_sap_subband(plot_files)
    sb_list    = sorted(sb_plots.keys())
    for sb_name in sb_list:
        file_name = os.path.join(sb_html_root, sb_name+'.html')
        open(file_name, 'w').write(
            subband_html(parset, sb_name, sb_plots[sb_name]))

    observation_index_name = os.path.join(obs_html_root, 'index.html')
    
    subband_list     = parset['subband_list']
    olap_subbands    = [int(sap_sb.split('_')[1][-3:]) for sap_sb in sb_list]
    station_subbands = [subband_list[olap_sb] for olap_sb in olap_subbands]
    subband_freqs_hz = [parset['subband_freqs_hz'][sb] for sb in olap_subbands]
    sb_page_list = ['<tr><td><a href="SBpages/%s.html">%s</a>&nbsp;&nbsp;&nbsp;</td> <td>%d&nbsp;&nbsp;&nbsp;</td> <td>%7.3f&nbsp;&nbsp;&nbsp;</td></tr>' % (name, name, subband, freq_hz/1e6)
                    for name, subband, freq_hz in zip(sb_list, station_subbands, subband_freqs_hz)]
    open(observation_index_name, 'w').write('''
<html>
    <head><title>%(sas_prefix)s%(sas_id)s</title></head>
    <body>
    <h1>%(sas_prefix)s%(sas_id)s</h1>
    <p><ul>
        <li><a href="../index.html">Projects</a></li>
    </ul></p>

    %(file_size_info)s

    <table>
    <tr><th>Name</th> <th>Subband</th> <th>Freq.</th></tr>
    <tr><th></th>     <th>(ID)</th> <th>(MHz)</th></tr>
    %(sub_band_pages)s
    </table>
    </body>
</html>
''' % {'sas_prefix'    : parset['sas_prefix'],
       'sas_id'        : parset['sas_id'],
       'file_size_info': file_size_html,
       'sub_band_pages': ' '.join(sb_page_list)})


def parse_subband_list(parset_subband_list):
    r'''
    Parse a subband list from a parset.

    **Parameters**
    
    parset_subband_list : string
        Value of Observation.Beam[0].subbandList

    **Returns**
    
    A list of integers containing the subband numbers.
    
    **Examples**
    
    
    >>> parse_subband_list('[154..163,185..194,215..224,245..254,275..284,305..314,335..344,10*374]')
    [154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 374, 374, 374, 374, 374, 374, 374, 374, 374, 374]
    >>> parse_subband_list('[77..87,116..127,155..166,194..205,233..243,272..282,311..321]')
    [77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321]
    >>> parse_subband_list('[]')
    []
    '''
    stripped_subband_list = parset_subband_list.strip('[] \n\t')
    if stripped_subband_list == '':
        return []
    sub_lists = [word.strip().split('..') for word in stripped_subband_list.split(',')]
    subbands = []
    for sub_list in sub_lists:
        if len(sub_list) == 1:
            multiplication = sub_list[0].split('*')
            if len(multiplication) == 2:
                subbands += [int(multiplication[1])]*int(multiplication[0])
            else:
                subbands.append(int(sub_list[0]))
        elif len(sub_list) == 2:
            subbands += range(int(sub_list[0]), int(sub_list[1])+1)
        else:
            logging.error('%r is not a valid sub_range in a subband list', sub_list)
            return []
    return subbands


def subband_frequencies(subband_width, band_filter, subband_numbers):
    r'''
    '''
    freq_offsets_hz = [sb_id*subband_width for sb_id in subband_numbers]
    base_freq = 0.0
    if 'HBA_110_190' == band_filter:
        base_freq = 100e6
    elif 'HBA_210_250' == band_filter:
        base_freq = 200e6
    elif 'HBA_170_230' == band_filter:
        base_freq = 160e6
    else:
        base_freq = 0.0
    return [freq_hz + base_freq for freq_hz in freq_offsets_hz]


def find_parset(sas_id):
    r'''
    '''
    search_path = [os.path.join('/globalhome', user, 'log',
                                prefix+str(sas_id)+'.parset')
                   for prefix in ['L', 'T']
                   for user in ['lofarsystem', 'lofartest']]
    for path in search_path:
        if os.path.exists(path):
            return path
    logging.warn('Cannot find parset for SAS ID %d', sas_id)
    return None


def parset_summary(sas_id, parset_name):
    r'''
    Observation.antennaSet = LBA_INNER
    Observation.bandFilter = LBA_10_90
    Observation.startTime = '2012-07-03 14:44:44'
    Observation.stopTime = '2012-07-03 14:46:44'
    Observation.Beam[0].target = '3C 196'
    Observation.Beamformer.stationList = []
    Observation.Campaign.CO_I = 'Pizzo, Dr. Roberto Francesco'
    Observation.Campaign.PI = 'Polatidis, Dr Antonios'
    Observation.Campaign.contact = 'Polatidis, Dr Antonios'
    Observation.Campaign.name = 2012LOFAROBS
    Observation.Campaign.title = 2012LOFAROBS
    Observation.clockMode = <<Clock200
    Observation.channelsPerSubband = 64
    Observation.subbandList = [77..320]
    Observation.subbandWidth = 195.3125
    OLAP.nrBitsPerSample = 16
    Observation.DataProducts.Output_Correlated.filenames = [L178900_SAP000_SB000_uv.MS,L178900_SAP000_SB001_uv.MS,L178900_SAP000_SB002_uv.MS,L178900_SAP000_SB003_uv.MS,L178900_SAP000_SB004_uv.MS,L178900_SAP000_SB005_uv.MS,L178900_SAP000_SB006_uv.MS,L178900_SAP000_SB007_uv.MS]
    Observation.DataProducts.Output_Beamformed.filenames = [L181718_SAP000_B000_S0_P000_bf.raw,L181718_SAP000_B001_S0_P000_bf.raw,L181718_SAP000_B002_S0_P000_bf.raw,L181718_SAP000_B003_S0_P000_bf.raw,L181718_SAP000_B004_S0_P000_bf.raw,L181718_SAP000_B005_S0_P000_bf.raw,L181718_SAP000_B006_S0_P000_bf.raw]
    '''
    def value(line):
        return line.split('=')[-1].strip('\' \n')

    parset = {'sas_id'       : str(sas_id),
              'sas_prefix'   : '',
              'campaign_name': '',
              'target'       : '',
              'start_time'   : '',
              'stop_time'    : '',
              'antenna_set'  : '',
              'band_filter'  : '',
              'subband_list' : [],
              'subband_freqs_hz': [],
              'subband_width': 0.0,
              '200_subband_width': 0.0,
              '160_subband_width': 0.0,
              'file_name'    : None,
              'clock_mhz'    : 0,
              'correlator_products': [],
              'correlator_locations': [],
              'beamformer_products': [],
              'beamformer_locations': []
              }

   
    if parset_name and os.path.exists(parset_name):
        parset['file_name']  = parset_name
        parset['sas_prefix'] = os.path.basename(parset_name).split('.parset')[0][0]
        parset_lines = open(parset_name).readlines()
        for line in parset_lines:
            if 'Campaign.name' in line:
                parset['campaign_name'] = value(line)
            elif 'Beam[0].target' in line:
                parset['target'] = value(line)
            elif 'Observation.startTime' in line:
                parset['start_time'] = value(line)
            elif 'Observation.stopTime' in line:
                parset['stop_time'] = value(line)
            elif 'Observation.antennaSet' in line:
                parset['antenna_set'] = value(line)
            elif  'Observation.bandFilter' in line:
                parset['band_filter'] = value(line)
            elif 'Observation.subbandList' in line:
                subband_list = parse_subband_list(value(line))
                if len(subband_list) == 0:
                    logging.warn('Empty subband list in %s', parset_name)
                parset['subband_list'] = subband_list
            elif 'Observation.clockMode' in line:
                parset['clock_mhz'] = int(value(line).strip('<<Clock \n\t'))
            elif 'Observation.subbandWidth' in line:
                parset['subband_width'] = float(value(line))
            elif 'Clock200.subbandWidth' in line:
                parset['200_subband_width'] = float(value(line))
            elif 'Clock160.subbandWidth' in line:
                parset['160_subband_width'] = float(value(line))
            elif 'Observation.DataProducts.Output_Correlated.filenames' in line:
                parset['correlator_products'] = [name.strip()
                                                 for name in  value(line).strip('[] ').split(',')]
            elif 'Observation.DataProducts.Output_Correlated.locations' in line:
                parset['correlator_locations'] = [name.strip().split(':')[0]
                                                  for name in  value(line).strip('[] ').split(',')]
            elif 'Observation.DataProducts.Output_Beamformed.filenames' in line:
                parset['beamformer_products'] = [name.strip()
                                                 for name in  value(line).strip('[] ').split(',')]
            elif 'Observation.DataProducts.Output_Beamformed.locations' in line:
                parset['beamformer_locations'] = [name.strip().split(':')[0]
                                                  for name in  value(line).strip('[] ').split(',')]
            else:
                pass
    if parset['clock_mhz'] == 160:
        parset['subband_width'] = parset['160_subband_width']
    if parset['clock_mhz'] == 200:
        parset['subband_width'] = parset['200_subband_width']

    if parset['subband_width'] == 0.0:
        parset['subband_width'] = parset['clock_mhz']*1e6/1024.0/1000.0
        
    parset['subband_freqs_hz'] = subband_frequencies(parset['subband_width']*1000.0,
                                                     parset['band_filter'],
                                                     parset['subband_list'])
    return parset



def parse_file_sizes(file_name):
    r'''
    Parse the file-sizes.txt file normally residing at
    /globaldata/inspect/<sas_id>/file-sizes.txt.

    **Parameters**
    
    file_name : string
       Full path to the file.

    **Returns**
    
    A dictionary with the file name of the data set (without full
    path) as a key, and a tuple (locus_node, full_path_to_data,
    file_size_in_mb)

    **Examples**
    
    >>> cc_sizes = parse_file_sizes('testdata/crosscor-file-sizes.txt')
    >>> cc_sizes['L178900_SAP000_SB000_uv.MS']
    ('locus001', '/data/L178900/L178900_SAP000_SB000_uv.MS', 468)
    >>> cc_sizes['L178900_SAP000_SB243_uv.MS']
    ('locus092', '/data/L178900/L178900_SAP000_SB243_uv.MS', 468)
    >>> fe_sizes = parse_file_sizes('testdata/flyseye-file-sizes.txt')
    '''
    lines = [line.strip().split() for line in open(file_name).readlines()
             if ('No such file or directory' not in line and
                 '************'  not in line and
                 'Timeout, server not' not in line and
                 'connect to host' not in line and
                 'request failed' not in line and
                 'not resolve' not in line and
                 'remaining bytes' not in line)]
    results = {}
    locus_node = None
    for line in lines:
        if len(line) == 3 and 'locus' in line[1]:
            locus_node = line[1]
        elif len(line) == 2:
            data_size_mb = int(line[0])
            full_path    = line[1]
            file_name    = os.path.basename(full_path)
            results[file_name] = (locus_node, full_path, data_size_mb)
        else:
            logging.warning('parse_file_sizes(): incomprehensible line %r',
                            line)
    return results




def file_size_analysis(parset, file_sizes_dict):
    r'''
    Analyze the file sizes agains the expected files as listed in the parset.
    '''
    #logging.info('file_size_analysis(): %r', file_sizes_dict)
    ms_sizes  = array([value[2] for keyword, value in file_sizes_dict.iteritems()
                       if '.MS'  in keyword])
    raw_sizes = array([value[2] for keyword, value in file_sizes_dict.iteritems()
                       if '.raw' in keyword])
    #logging.info('file_size_analysis(): %r', ms_sizes)
    #logging.info('file_size_analysis(): %r', raw_sizes)

    median_ms_size_mb = 0
    if len(ms_sizes) > 0:
        median_ms_size_mb  = median(ms_sizes)
    median_raw_size_mb = 0
    if len(raw_sizes) > 0:
        median_raw_size_mb = median(raw_sizes)
    
    missing_data_sets   = []
    odd_sized_data_sets = []

    total_data_products = len(parset['correlator_products']) + len(parset['beamformer_products'])
    problematic_data_products = 0

    for msname, locus_node in zip(parset['correlator_products'], parset['correlator_locations']):
        node_and_name = '%s:%s' % (locus_node, msname)
        try:
            locus_actual, full_path, data_size_mb = file_sizes_dict[msname]
            if median_ms_size_mb > 0:
                if abs(float(data_size_mb)/float(median_ms_size_mb) -1.0) > 0.01:
                    odd_sized_data_sets.append((node_and_name, data_size_mb))
                    problematic_data_products += 1
        except KeyError:
            missing_data_sets.append(node_and_name)
            problematic_data_products += 1
                
    for raw_name, locus_node in zip(parset['beamformer_products'], parset['beamformer_locations']):
        node_and_name = '%s:%s' % (locus_node, raw_name)
        try:
            locus_actual, full_path, data_size_mb = file_sizes_dict[raw_name]
            if median_raw_size_mb > 0:
                if abs((float(data_size_mb)/float(median_raw_size_mb)) -1.0) > 0.01:
                    odd_sized_data_sets.append((node_and_name, data_size_mb))
                    problematic_data_products += 1
        except KeyError:
            missing_data_sets.append(node_and_name)
            problematic_data_products += 1
    
    percentage_complete = 0.0
    if total_data_products > 0:
        percentage_complete = floor(100.0*float(total_data_products - problematic_data_products)/total_data_products)

    return {'median_ms_size_mb': median_ms_size_mb,
            'median_raw_size_mb': median_raw_size_mb,
            'missing_data_sets': missing_data_sets,
            'odd_sized_data_sets': odd_sized_data_sets,
            'percentage_complete': percentage_complete}

    



def observation_table_row(parset, percentage_complete, html_root, ascii_table=False):
    r'''
    '''
    format_dict = parset.copy()
    format_dict['nr_subbands']         = len(parset['subband_list'])
    format_dict['percentage_complete'] = int(floor(percentage_complete))
    
    if parset['file_name']:
        format_dict['parset_relpath'] = os.path.relpath(
            parset['file_name'], html_root)
        html = '''
    <tr><th><a href="%(sas_id)s/index.html">%(sas_prefix)s%(sas_id)s</a>&nbsp;&nbsp;&nbsp;</th>
        <td>%(campaign_name)s&nbsp;&nbsp;&nbsp;</td> <td>%(target)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(antenna_set)s&nbsp;&nbsp;&nbsp;</td>   <td>%(band_filter)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(start_time)s&nbsp;&nbsp;&nbsp;</td>    <td>%(stop_time)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(clock_mhz)d&nbsp;&nbsp;&nbsp;</td>     <td>%(nr_subbands)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(percentage_complete)d%%</td>
        <td><a href="%(parset_relpath)s">parset</a>&nbsp;&nbsp;&nbsp;</td>
    </tr>
''' % format_dict

    else:
        html = '''
    <tr><th><a href="%(sas_id)s/index.html">%(sas_prefix)s%(sas_id)s</a>&nbsp;&nbsp;&nbsp;</th>
        <td>%(campaign_name)s&nbsp;&nbsp;&nbsp;</td> <td>%(target)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(antenna_set)s&nbsp;&nbsp;&nbsp;</td>   <td>%(band_filter)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(start_time)s&nbsp;&nbsp;&nbsp;</td>    <td>%(stop_time)s&nbsp;&nbsp;&nbsp;</td>
        <td>%(clock_mhz)d&nbsp;&nbsp;&nbsp;</td>     <td>%(nr_subbands)s&nbsp;&nbsp;&nbsp;</td>
        <td>???</td>
        <td>no parset&nbsp;&nbsp;&nbsp;</td>
    </tr>
''' % format_dict

    if ascii_table:
        html = '''%(sas_prefix)s%(sas_id)s  %(campaign_name)8s  %(target)19s  %(antenna_set)16s  %(band_filter)11s  %(start_time)s -- %(stop_time)s %(percentage_complete)d%%''' % format_dict

    return html



def main_index_html(plot_root, html_root):
    r'''
    '''
    logging.debug('main_index_html(plot_root = %r, html_root = %r)',
                  plot_root, html_root)

    index_name      = os.path.join(html_root, 'index.html')
    index_txt_name  = os.path.join(html_root, 'index.txt')
    long_index_name = os.path.join(html_root, 'fullindex.html')
    beginning_of_short_index = datetime.datetime.now() - datetime.timedelta(7)

    plot_sas_id_pattern5 = os.path.join(plot_root, '[0123456789]'*5)
    plot_sas_id_pattern6 = os.path.join(plot_root, '[0123456789]'*6)
    plot_sas_id_pattern7 = os.path.join(plot_root, '[0123456789]'*7)
    sas_id_with_plot = [int(os.path.split(sas_id)[-1])
                        for sas_id in (glob.glob(plot_sas_id_pattern5)
                                       + glob.glob(plot_sas_id_pattern6)
                                       + glob.glob(plot_sas_id_pattern7))]

    html_sas_id_pattern5 = os.path.join(html_root, '[0123456789]'*5)
    html_sas_id_pattern6 = os.path.join(html_root, '[0123456789]'*6)
    html_sas_id_pattern7 = os.path.join(html_root, '[0123456789]'*7)
    sas_id_with_html = [int(os.path.split(sas_id)[-1]) 
                        for sas_id in (glob.glob(html_sas_id_pattern5)
                                       + glob.glob(html_sas_id_pattern6)
                                       + glob.glob(html_sas_id_pattern7))]
    sas_ids = sorted(set(sas_id_with_html).intersection(set(sas_id_with_plot)),
                     reverse = True)
    unsorted_parsets = [parset_summary(sas_id, find_parset(sas_id))
                        for sas_id in sas_ids]
    parsets = sorted(unsorted_parsets,
                     key     = lambda parset: parset['start_time'],
                     reverse = True)
    file_sizes = []
    for parset in parsets:
        obs_plot_root = os.path.join(plot_root, parset['sas_id'])
        if parset['file_name']:
            new_parset_name = os.path.join(obs_plot_root,
                                           os.path.basename(parset['file_name']))
        else:
            new_parset_name = None
        if parset['file_name'] and os.path.exists(parset['file_name']) \
                and not os.path.exists(new_parset_name):
            force_mkdir(obs_plot_root)
            shutil.copy2(parset['file_name'], obs_plot_root)
            
        if new_parset_name and os.path.exists(new_parset_name):
            parset['file_name'] = new_parset_name
        else:
            parset['file_name'] = None

        file_size_data = None
        file_sizes_txt = os.path.join(obs_plot_root, 'file-sizes.txt')
        if os.path.exists(file_sizes_txt):
            file_sizes_dict = parse_file_sizes(file_sizes_txt)
            file_size_data  = file_size_analysis(parset, file_sizes_dict)
        file_sizes.append(file_size_data)


    open(index_txt_name, 'w').write('\n'.join(
            [observation_table_row(parset, file_size['percentage_complete'],
                                   html_root, ascii_table=True)
             for parset, file_size in zip(parsets, file_sizes)
             if parset['start_time'].strip() != '' and datetime.datetime.strptime(parset['start_time'], '%Y-%m-%d %H:%M:%S') > beginning_of_short_index]))

    open(index_name, 'w').write('''
<html>
    <head>
        <meta http-equiv="refresh" content="60">
        <title>LOFAR Inspection plots</title>
    </head>
    <body>
        <h1>LOFAR inspection plots</h1>
        <p><i>Last modified: %s UTC</i>&nbsp;&nbsp;<a href="%s">Full list</a>&nbsp;&nbsp;<a href="%s">Ascii table</a></p>
        <table>
        <tr><th>SAS ID</th> <th>Campaign</th> <th>Target</th> <th>AntennaSet</th> <th>Band</th> <th>Start</th> <th>End</th> <th>Clock</th> <th>Subb</th> <th>Compl</th> <th>Parset</th></tr>
        %s
        </table>
    </body>
</html>
''' % ( time.asctime(time.gmtime()),
        os.path.basename(long_index_name),
        os.path.basename(index_txt_name),
        '\n'.join([observation_table_row(parset, file_size['percentage_complete'], html_root)
                   for parset, file_size in zip(parsets, file_sizes)
                   if parset['start_time'].strip() != '' and datetime.datetime.strptime(parset['start_time'], '%Y-%m-%d %H:%M:%S') > beginning_of_short_index]
                  )))

    open(long_index_name, 'w').write('''
<html>
    <head>
        <meta http-equiv="refresh" content="60">
        <title>LOFAR Inspection plots</title>
    </head>
    <body>
        <h1>LOFAR inspection plots</h1>
        <p><i>Last modified: %s UTC</i>&nbsp;&nbsp;<a href="%s">Short index</a><p>
        <table>
        <tr><th>SAS ID</th> <th>Campaign</th> <th>Target</th> <th>AntennaSet</th> <th>Band</th> <th>Start</th> <th>End</th> <th>Clock</th> <th>Subb</th> <th>Compl</th> <th>Parset</th></tr>
        %s
        </table>
    </body>
</html>
''' % ( time.asctime(time.gmtime()),
        os.path.basename(index_name),
        '\n'.join([observation_table_row(parset, file_size['percentage_complete'], html_root)
                   for parset, file_size in zip(parsets, file_sizes)]
                  )))
    

if __name__ == '__main__':
    try:
        sys.exit(create_html_main(sys.argv))
    except SystemExit:
        raise
    except:
        logging.error('%s: %s', str(sys.exc_info()[0].__name__), str(sys.exc_info()[1]))
        sys.exit(2)
